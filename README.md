# Gradient Descent Algorithms Using Theano
Various gradient descent algorithms tested using `Theano`. For simplicity each algorithm has its own `.py` file. A jupyter notebook is  included explaining how each gradient descent method operates and how to generate the code for each instance. 

# References
<ol>
<li>Rumelhart D, Hinton G. E, Williams R. J, (8 October 1986). <em>Learning representations by back-propagating errors</em>. Nature. 323 (6088): 533–536. doi:10.1038/323533a0.</li>
<li>Sutskever I, et. all, <em>On the importance of initialization and momentum in deep learning</em>, In Proceedings of the 30th international conference on machine learning (ICML-13). 28. Atlanta, GA. pp. 1139–1147.</li>
<li>Duchi J, Hazan E, Singer Y (2011). <em>Adaptive subgradient methods for online learning and stochastic optimization</em>. JMLR. 12: 2121–2159.</li>
<li>Tieleman T, and Hinton G (2012). <em>Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</em>. COURSERA: Neural Networks for Machine Learning.</li>
<li>Diederik K, Ba J (2014). <em>Adam: A method for stochastic optimization</em>. arXiv:1412.6980.</li>
</ol>
